import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from tqdm.notebook import tqdm  # ä½¿ç”¨ notebook ä¸“ç”¨çš„è¿›åº¦æ¡ï¼Œå‡å°‘å¡é¡¿é£é™©
import matplotlib.pyplot as plt
import time
import os

# =================é…ç½®åŒºåŸŸ=================
# ä½ çš„ RTX 5060 åº”è¯¥èƒ½è½»æ¾å¤„ç† 128 æˆ– 256 çš„ Batch Size
BATCH_SIZE = 128
EPOCHS = 5  # æµ‹è¯•è¿è¡Œ 5 è½®å³å¯ï¼Œæƒ³è·‘å®Œæ•´è®­ç»ƒå¯ä»¥æ”¹æˆ 50
LEARNING_RATE = 0.01
NUM_WORKERS = 2  # WSL2 ä¸‹å»ºè®®ä¸è¦è®¾ç½®å¤ªé«˜ï¼Œ2-4 ä¹‹é—´ä¸ºå®œ


# =========================================

def get_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")


device = get_device()
print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")
    print(f"   æ˜¾å­˜æ€»é‡çš„: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

# 1. æ•°æ®å‡†å¤‡ (ä½¿ç”¨ CIFAR-10)
# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½çº¦ 160MB çš„æ•°æ®
print("\nğŸ“¦ æ­£åœ¨å‡†å¤‡æ•°æ®...")
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# å»ºè®®å°† root æ”¹ä¸ºä½  D ç›˜çš„æŒ‚è½½è·¯å¾„ï¼Œæˆ–è€…ç›´æ¥ç”¨ç›¸å¯¹è·¯å¾„
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

# 2. å®šä¹‰æ¨¡å‹ (ResNet18)
print("ğŸ—ï¸ æ­£åœ¨åŠ è½½ ResNet18 æ¨¡å‹...")
# ä¿®æ”¹ num_classes=10 é€‚é… CIFAR-10
model = torchvision.models.resnet18(weights=None, num_classes=10)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)
scaler = torch.amp.GradScaler('cuda')  # æ··åˆç²¾åº¦è®­ç»ƒï¼Œåˆ©ç”¨ RTX æ˜¾å¡çš„ Tensor Cores

# 3. è®­ç»ƒå¾ªç¯
print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ (å…± {EPOCHS} è½®)...")
train_losses = []
train_accs = []

start_time = time.time()

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œdesc ç”¨äºæ˜¾ç¤ºå½“å‰ Epoch ä¿¡æ¯
    pbar = tqdm(trainloader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch')

    for inputs, labels in pbar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        # å¼€å¯æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
        with torch.amp.autocast('cuda'):
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        # æ›´æ–°è¿›åº¦æ¡åç¼€ï¼Œæ˜¾ç¤ºå®æ—¶ Loss å’Œ Accuracy
        pbar.set_postfix({'Loss': running_loss / (pbar.n + 1), 'Acc': 100. * correct / total})

    epoch_loss = running_loss / len(trainloader)
    epoch_acc = 100. * correct / total
    train_losses.append(epoch_loss)
    train_accs.append(epoch_acc)

end_time = time.time()
print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

# 4. ç»˜åˆ¶ç»“æœ (æµ‹è¯•ç»˜å›¾æ˜¯å¦å¡é¡¿)
print("ğŸ“Š æ­£åœ¨ç»˜åˆ¶ç»“æœ...")
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.title('Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Training Accuracy', color='orange')
plt.title('Accuracy Curve')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.tight_layout()
plt.show()

print("ğŸ‰ å…¨éƒ¨ä»»åŠ¡ç»“æŸ")